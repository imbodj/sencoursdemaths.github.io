\input{../common}

\begin{document}
  %<*content>
  \lesson{analysis}{266}{Utilisation de la notion d'indépendance en probabilités.}

  Soit $(\Omega, \mathcal{A}, \mathbb{P})$ un espace probabilisé.

  \subsection{Indépendance en probabilités}

  \subsubsection{Indépendance d'événements}

  \reference[G-K]{52}

  \begin{definition}
    On dit que deux événements $A$ et $B$ sont \textbf{indépendants} (sous $\mathbb{P}$) si
    \[ \mathbb{P}(A \, \cap \, B) = \mathbb{P}(A) \mathbb{P}(B) \]
  \end{definition}

  \begin{definition}
    On dit que les événements d'une famille $(A_i)_{i \in I}$ sont \textbf{mutuellement indépendants} si
    \[ \forall J \subseteq I, \, J \text{ fini}, \, \mathbb{P}\left( \bigcap_{j \in J} A_j \right) = \prod_{j \in J} \mathbb{P}(A_j) \]
  \end{definition}

  \reference[DAN]{425}

  \begin{proposition}
    Soient $A, B$ deux événements. Alors,
    \[ A \text{ et } B \text{ sont indépendants} \iff \mathbb{P}(A \setminus B) = \mathbb{P}(A) \iff \mathbb{P}(B \setminus A) = \mathbb{P}(B) \]
  \end{proposition}

  \begin{proposition}
    Soient $A_1, \dots, A_n$ des événements mutuellement indépendants. Alors, pour tout $k \in \llbracket 1, n \rrbracket$, $A_1^c, \dots, A_k^c, A_{k+1}, \dots, A_n$ sont mutuellement indépendants.
  \end{proposition}

  \begin{example}
    On considère deux gênes $a$ et $b$ tels que la redondance de l'un d'entre eux entraîne l'acquisition d'un caractère d'un caractère $\mathcal{C}$. Anselme et Colette possèdent chacun la combinaison $ab$ et attendant un enfant : elles lui transmettront chacun et indépendamment soit le gêne $a$, soit le gêne $b$ avec la même probabilité de $\frac{1}{2}$. On considère les événements :
    \begin{itemize}
      \item $A$ : Colette transmet le gêne $a$.
      \item $B$ : Anselme transmet le gêne $b$.
      \item $C$ : l'enfant présent le caractère $\mathcal{C}$.
    \end{itemize}
    $A$, $B$ et $C$ sont indépendants deux à deux, mais non mutuellement indépendants.
  \end{example}

  \begin{application}[Indicatrice d'Euler]
    On note $\varphi$ l'indicatrice d'Euler. Alors,
    \[ \forall n \geq 2, \, \varphi(n) = n \prod_{\substack{p \text{ premier} \\ p \mid n}} \left( 1 - \frac{1}{p} \right) \]
  \end{application}

  \subsubsection{Indépendance de tribus}

  \reference[G-K]{52}

  \begin{definition}
    On dit que deux sous-tribus $\mathcal{A}_1$ et $\mathcal{A}_2$ de $\mathcal{A}$ sont \textbf{indépendantes} (sous $\mathbb{P}$) si
    \[ \forall A \in \mathcal{A}_1, \, \forall B \in \mathcal{A}_2, \, \mathbb{P}(A \, \cap \, B) = \mathbb{P}(A) \mathbb{P}(B) \]
  \end{definition}

  \begin{definition}
    On dit qu'une famille de sous-tribus $(\mathcal{A}_i)_{i \in I}$ de $\mathcal{A}$ sont \textbf{indépendantes} (sous $\mathbb{P}$) si
    \[ \forall J \subseteq I, \, J \text{ fini}, \, \forall (A_j)_{j \in J} \in \prod_{j \in J} \mathcal{A}_j, \, \mathbb{P}\left( \bigcap_{j \in J} A_j \right) = \prod_{j \in J} \mathbb{P}(A_j) \]
  \end{definition}

  \begin{remark}
    \begin{itemize}
      \item Pour tout $A, B \in \mathcal{A}$, $A$ est indépendante de $B$ si et seulement si $\sigma(A)$ est indépendante de $\sigma(B)$.
      \item Si deux tribus $\mathcal{A}$ et $\mathcal{B}$ sont indépendantes, toute sous tribu de $\mathcal{A}$ est indépendante de toute sous tribu de $\mathcal{B}$.
    \end{itemize}
  \end{remark}

  \subsubsection{Indépendance de variables aléatoires}

  \paragraph{Variables aléatoires indépendantes}

  \reference{125}

  \begin{definition}
    Soit $X$ une variable aléatoire réelle définie sur $(\Omega, \mathcal{A}, \mathbb{P})$. On note
    \[ \sigma(X) = \{ X^{-1}(A) \mid A \in \mathcal{B}(\mathbb{R}) \} \]
    Cette famille est la \textbf{tribu engendrée} par $X$.
  \end{definition}

  \begin{definition}
    On dit que deux variables aléatoires $X$ et $Y$ sont \textbf{indépendantes} si les tribus qu'elles engendrent sont indépendantes.
  \end{definition}

  \begin{example}
    Si $X$ et $Y$ sont deux variables aléatoires indépendantes, on a
    \[ \forall A, B \in \mathcal{B}(\mathbb{R}), \, \mathbb{P}(\{ X \in A \} \, \cap \, \{ Y \in B \}) = \mathbb{P}(X \in A) \mathbb{P}(Y \in B) \]
  \end{example}

  \begin{proposition}
    Si $X$ et $Y$ sont deux variables aléatoires indépendantes, alors $f(X)$ et $g(Y)$ sont indépendantes pour toutes fonctions mesurables $f$ et $g$.
  \end{proposition}

  \reference{136}

  \begin{proposition}
    Soient $X$ et $Y$ deux vecteurs aléatoires indépendants. On suppose que $X$ admet une densité $f$ et $Y$ admet une densité $g$. Alors, $(X,Y)$ admet comme densité $(x,y) \mapsto f(x)g(y)$.
  \end{proposition}

  \reference{175}

  \begin{proposition}
    Soient $X$ et $Y$ deux vecteurs aléatoires indépendants intégrables. Alors,
    \[ \mathbb{E}(XY) = \mathbb{E}(X) \mathbb{E}(Y) \]
  \end{proposition}

  \paragraph{Variables aléatoires non corrélées}

  \reference{174}

  \begin{definition}
    On dit que deux variables aléatoires $X$ et $Y$ sont \textbf{non corrélées} si
    \[ \operatorname{Covar}(X,Y) = \mathbb{E}(X - \mathbb{E}(X))\mathbb{E}(Y - \mathbb{E}(Y)) = 0 \]
  \end{definition}

  \begin{proposition}
    Soient $X$ et $Y$ deux variables aléatoires indépendantes intégrables. Alors $X$ et $Y$ ne sont pas corrélées.
  \end{proposition}

  \begin{cexample}
    La réciproque est fausse. Ainsi, soient $X$ et $Y$ deux variables aléatoires vérifiant
    \begin{align*}
      \mathbb{P}(\{ X = 1 \} \, \cap \, \{ Y = 1 \}) &= \mathbb{P}(\{ X = 1 \} \, \cap \, \{ Y = -1 \}) \\
      &= \mathbb{P}(\{ X = -1 \} \, \cap \, \{ Y = 0 \}) \\
      &= \frac{1}{3}
    \end{align*}
    alors, $X$ et $Y$ sont non corrélées mais pas indépendantes.
  \end{cexample}

  \subsection{Étude de variables aléatoires indépendantes}

  \subsubsection{Critères d'indépendance}

  \reference{128}

  \begin{theorem}
    Soient $X$ et $Y$ deux variables aléatoires. Alors, $X$ et $Y$ sont indépendantes si et seulement si $\mathbb{P}_{(X,Y)} = \mathbb{P}_X \otimes \mathbb{P}_Y$.
  \end{theorem}

  \begin{corollary}
    Soient $X$ et $Y$ deux variables aléatoires indépendantes. Alors, $\mathbb{P}_{X+Y} = \mathbb{P}_X * \mathbb{P}_Y$.
  \end{corollary}

  \reference{136}

  \begin{proposition}
    Soient $X$ et $Y$ deux variables aléatoires définies sur $(\Omega, \mathcal{A}, \mathbb{P})$. On suppose que $(X,Y)$ admet une densité $h : (x,y) \mapsto f(x) g(y)$ à variables séparées. Alors, $X$ et $Y$ sont indépendantes. De plus, $X$ et $Y$ admettent respectivement pour densité
    \[ x \mapsto \frac{f(x)}{\int_{\mathbb{R}} f(t) \, \mathrm{d}t} \text{ et } y \mapsto \frac{g(y)}{\int_{\mathbb{R}} g(t) \, \mathrm{d}t} \]
    par rapport à la mesure de Lebesgue.
  \end{proposition}

  \subsubsection{Sommes de variables aléatoires indépendantes}

  \reference{179}

  \begin{theorem}
    Soient $X$ et $Y$ deux variables aléatoires réelles indépendantes de densités respectives $f$ et $g$. Alors, $X + Y$ admet comme densité la fonction $f * g : x \mapsto \int_{\mathbb{R}} f(x-t) g(t) \, \mathrm{d}t$.
  \end{theorem}

  \begin{application}
    Soient $X$ et $Y$ deux variables aléatoires indépendantes telles que $X \sim \Gamma(a, \gamma)$ et $Y \sim \Gamma(b, \gamma)$. Alors $Z = X + Y \sim \Gamma(a+b, \gamma)$.
  \end{application}

  \begin{application}
    \[ \frac{\Gamma(a) \Gamma(b)}{\Gamma(a+b)} = \int_0^1 \theta^{a-1} (1-\theta)^{b-1} \, \mathrm{d}\theta \]
    où $\Gamma$ désigne la fonction $\Gamma$ d'Euler.
  \end{application}

  \reference{235}

  \begin{definition}
    Soit $X$ une variable aléatoire à valeurs dans $\mathbb{N}$. On appelle \textbf{fonction génératrice} de $X$ la fonction
    \[
    G_X :
    \begin{array}{ccc}
      [-1,1] &\rightarrow& \mathbb{R} \\
      z &\mapsto& \sum_{k=0}^{+\infty} \mathbb{P}(X=k) z^k
    \end{array}
    \]
  \end{definition}

  \begin{proposition}
    Soient $X$ et $Y$ deux variables aléatoires à valeurs dans $\mathbb{N}$ indépendantes. Alors,
    \[ G_{X+Y} = G_X G_Y \]
  \end{proposition}

  \begin{theorem}
    Sur $[0,1]$, la fonction $G_X$ est infiniment dérivable et ses dérivées sont toutes positives, avec
    \[ G_X^{(n)}(s) = \mathbb{E}(X(X-1) \dots (X-n+1)s^{X-n}) \]
    En particulier,
    \[ \mathbb{P}(X=n) = \frac{G_X^{(n)}(0)}{n!} \]
    ce qui montre que la fonction génératrice caractérise la loi.
  \end{theorem}

  \begin{example}
    Si $X_1 \sim \mathcal{P}(\lambda)$ et $X_2 \sim \mathcal{P}(\mu)$ sont indépendantes, alors $X_1 + X_2 \sim \mathcal{P}(\lambda + \mu)$.
  \end{example}

  \reference[GOU21]{346}

  \begin{example}
    Si $X_1 \sim \mathcal{B}(n, p)$ et $X_2 \sim \mathcal{B}(m, p)$ sont indépendantes, alors $X_1 + X_2 \sim \mathcal{B}(n + m, p)$.
  \end{example}

  \reference[G-K]{239}

  \begin{definition}
    On appelle \textbf{fonction caractéristique} de $X$ la fonction $\phi_X$ définie sur $\mathbb{R}^d$ par
    \[ \phi_X : t \mapsto \mathbb{E}\left( e^{i \langle t, X \rangle} \right) \]
  \end{definition}

  \begin{theorem}
    Si deux variables (ou vecteurs) aléatoires ont la même fonction caractéristique, alors elles ont même loi.
  \end{theorem}

  \begin{proposition}
    Si deux variables aléatoires réelles sont indépendantes, alors $\phi_{X+Y} = \phi_X \phi_Y$.
  \end{proposition}

  \subsection{Indépendance et théorèmes limites}

  \subsubsection{Lemmes de Borel-Cantelli}

  \reference{272}

  \begin{theorem}[1\ier{} lemme de Borel-Cantelli]
    Soit $(A_n)$ une suite d'événements. Si $\sum \mathbb{P}(A_n)$ converge, alors
    \[ \mathbb{P} \left( \limsup_{n \rightarrow +\infty} A_n \right) = 0 \]
  \end{theorem}

  \begin{remark}
    Cela signifie que presque sûrement, seul un nombre fini d'événements $A_n$ se réalisent.
  \end{remark}

  \begin{corollary}
    Si $\sum \mathbb{P}(\vert X_n - X \vert > \epsilon)$ converge pour tout $\epsilon > 0$, alors $X_n \overset{(ps.)}{\longrightarrow} X$.
  \end{corollary}

  \reference{285}

  \begin{example}
    Si $(X_n)$ est telle que $\forall n \geq 1$, $\mathbb{P}(X_n = n) = \mathbb{P}(X_n = \pm n) = \frac{1}{2n^2}$ et $\mathbb{P}(X_n = 0) = 1 - \frac{1}{2n^2}$, alors la suite $(S_n)$ définie pour tout $n \geq 1$ par $S_n = \sum_{k=1}^n X_k$ est constante à partir d'un certain rang.
  \end{example}

  \reference{273}

  \begin{theorem}[2\ieme{} lemme de Borel-Cantelli]
    Soit $(A_n)$ une suite d'événements indépendants. Si $\sum \mathbb{P}(A_n)$ diverge, alors
    \[ \mathbb{P} \left( \limsup_{n \rightarrow +\infty} A_n \right) = 1 \]
  \end{theorem}

  \begin{remark}
    Cela signifie que presque sûrement, un nombre infini d'événements $A_n$ se réalisent.
  \end{remark}

  \reference{286}

  \begin{example}
    On fait une infinité de lancers d'une pièce de monnaie équilibrée. Alors, la probabilité de l'événement ``on obtient une infinité de fois deux ``Face'' consécutifs'' est $1$.
  \end{example}

  \begin{corollary}[Loi du $0$-$1$ de Borel]
    Soit $(A_n)$ une suite d'événements indépendants, alors
    \[ \mathbb{P} \left( \limsup_{n \rightarrow +\infty} A_n \right) = 0 \text{ ou } 1 \]
    et elle vaut $1$ si et seulement si $\sum \mathbb{P}(A_n)$ diverge.
  \end{corollary}

  \subsubsection{Lois des grands nombres}

  \reference{270}

  \begin{theorem}[Loi faible des grands nombres]
    Soit $(X_n)$ une suite de variables aléatoires deux à deux indépendantes de même loi et $\mathcal{L}_1$. On pose $M_n = \frac{X_1 + \dots + X_n}{n}$. Alors,
    \[ M_n \overset{(p)}{\longrightarrow} \mathbb{E}(X_1) \]
  \end{theorem}

  \reference[Z-Q]{532}

  \begin{theorem}[Loi forte des grands nombres]
    Soit $(X_n)$ une suite de variables aléatoires mutuellement indépendantes de même loi. On pose $M_n = \frac{X_1 + \dots + X_n}{n}$. Alors,
    \[ X_1 \in \mathcal{L}_1 \iff M_n \overset{(ps.)}{\longrightarrow} \ell \in \mathbb{R} \]
    Dans ce cas, on a $\ell = \mathbb{E}(X_1)$.
  \end{theorem}

  \reference[G-K]{195}

  \begin{application}[Théorème de Bernstein]
    Soit $f : [0,1] \rightarrow \mathbb{R}$ continue. On note
    \[ \forall n \in \mathbb{N}^*, \, B_n(f) : x \mapsto \sum_{k=0}^n \binom{n}{k} f \left( \frac{k}{n} \right) x^k (1-x)^{n-k} \]
    le $n$-ième polynôme de Bernstein associé à $f$. Alors la suite de fonctions $(B_n(f))$ converge uniformément vers $f$.
  \end{application}

  \dev{theoreme-de-weierstrass-par-les-probabilites}

  \begin{corollary}[Théorème de Weierstrass]
    Toute fonction continue $f : [a,b] \rightarrow \mathbb{R}$ (avec $a, b \in \mathbb{R}$ tels que $a \leq b$) est limite uniforme de fonctions polynômiales sur $[a, b]$.
  \end{corollary}

  \subsubsection{Théorème central limite}

  \reference[Z-Q]{544}

  \begin{theorem}[Lévy]
    \label{theoreme-central-limite-1}
    Soient $(X_n)$ une suite de variables aléatoires réelles et $X$ une variable aléatoire réelle. Alors :
    \[ X_n \overset{(d)}{\longrightarrow} X \iff \phi_{X_n} \text{ converge simplement vers } \phi_X \]
  \end{theorem}

  \reference[G-K]{307}

  \begin{theorem}[Central limite]
    On suppose que $(X_n)$ est une suite de variables aléatoires réelles indépendantes de même loi admettant un moment d'ordre $2$. On note $m$ l'espérance et $\sigma^2$ la variance commune à ces variables. On pose $S_n = X_1 + \dots + X_n - nm$. Alors,
    \[ \left ( \frac{S_n}{\sqrt{n}} \right) \overset{(d)}{\longrightarrow} \mathcal{N}(0, \sigma^2) \]
  \end{theorem}

  \begin{application}[Théorème de Moivre-Laplace]
    On suppose que $(X_n)$ est une suite de variables aléatoires indépendantes de même loi $\mathcal{B}(p)$. Alors,
    \[ \frac{\sum_{k=1}^{n} X_k - np}{\sqrt{n}} \overset{(d)}{\longrightarrow} \mathcal{N}(0, p(1-p)) \]
  \end{application}

  \reference{556}

  \begin{application}[Formule de Stirling]
    \[ n! \sim \sqrt{2n\pi} \left(\frac{n}{e} \right)^n \]
  \end{application}

  \reference{390}
  \dev{theoreme-des-evenements-rares-de-poisson}

  \begin{application}[Théorème des événements rares de Poisson]
    Soit $(N_n)_{n \geq 1}$ une suite d'entiers tendant vers l'infini. On suppose que pour tout $n$, $A_{n,N_1}, \dots , A_{n,N_n}$ sont des événements indépendants avec $\mathbb{P}(A_{n,N_k}) = p_{n,k}$. On suppose également que :
    \begin{enumerate}[label=(\roman*)]
      \item $\lim_{n \rightarrow +\infty} s_n = \lambda > 0$ où $\forall n \in \mathbb{N}, s_n = \sum_{k=1}^{N_n} p_{n,k}$.
      \item $\lim_{n \rightarrow +\infty} \sup_{k \in \llbracket 1, N_n \rrbracket} p_{n,k} = 0$.
    \end{enumerate}
    Alors, la suite de variables aléatoires $(S_n)$ définie par
    \[ \forall n \in \mathbb{N}^*, \, S_n = \sum_{k=1}^n \mathbb{1}_{A_{n,k}} \]
    converge en loi vers la loi de Poisson de paramètre $\lambda$.
  \end{application}

  \annexessection

  \reference[G-K]{137}
  \reference{236}

  \begin{figure}[H]
    \begin{center}
      \begin{whitetabularx}{|X|X|}
        \hline
        \textbf{Loi} & \textbf{Somme} (indépendantes de même loi) \\
        \hline
        de Bernoulli & $\sum_{k=1}^n \mathcal{B}(p) \sim \mathcal{B}(n,p)$ \\
        \hline
        Binomiale & $\sum_{k=1}^n \mathcal{B}(n_k, p) \sim \mathcal{B} \left( \sum_{k=1}^n n_k, p \right)$ \\
        \hline
        de Poisson & $\sum_{k=1}^n \mathcal{P}(\lambda_k) \sim \mathcal{P} \left( \sum_{k=1}^n \lambda_k \right)$ \\
        \hline
      \end{whitetabularx}
    \end{center}
    \caption{Sommes de variables aléatoires à lois discrètes.}
  \end{figure}

  \reference{142}
  \reference{247}
  \reference{178}

  \begin{figure}[H]
    \begin{center}
      \begin{whitetabularx}{|X|X|}
        \hline
        \textbf{Loi} & \textbf{Somme} (indépendantes de même loi) \\
        \hline
        Normale & $\sum_{k=1}^n \mathcal{N}(\mu_k, \sigma_k^2) \sim \mathcal{N} \left( \sum_{k=1}^n \mu_k, \sum_{k=1}^n \sigma_k^2 \right)$ \\
        \hline
        Exponentielle & $\sum_{k=1}^n \mathcal{E}(\lambda) \sim \mathcal{E} \left( n\lambda \right)$ \\
        \hline
        Gamma & $\sum_{k=1}^n \Gamma(a_k, \gamma) \sim \Gamma \left( \sum_{k=1}^n a_k, \gamma \right)$ \\
        \hline
      \end{whitetabularx}
    \end{center}
    \caption{Sommes de variables aléatoires à densité.}
  \end{figure}
  %</content>
\end{document}
